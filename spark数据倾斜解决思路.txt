首先找到造成数据倾斜的shuffle操作，找到后进行一下的考量：

1、将造成数据倾斜的提前，提交到hive中，对hive进行预处理，让hive来进行聚合操作

2、过滤掉造成数据倾斜的数据，造成数据倾斜的key很少且不重要

3、提高shuffle的并行度，默认为200，spark sql中使用spark.sql.shuffler.partitions参数设置

4、使用随机数，对key进行添加随机数前缀，这样一个key就会变成多个key，再进行局部聚合(reduce)时，去掉前面的随机数，之后再次使用全局聚合来进行统计(分区中操作局部聚合)

5、将reduce join转换为map join，将较少数据的RDD或表进行广播(collect()后broadcast())，之后将之前造成数据倾斜的shuffler算子使用map替代，map中的每个数据都和广播中的数据进行对比key，相同的key则是要求进行的join操作

6、采样倾斜数据的key(join操作时)，取倾斜RDD中数据倾斜的key成独立的一个RDD，这些key都添加一个随机数的前缀，将正常RDD中和倾斜的key(也就是前面从倾斜RDD中独立出来的RDD中的key)相同的key独立为一个RDD，并对其进行膨胀(添加前缀，可以随机或顺序)，之后再使用从正常RDD中独立出来膨胀后的RDD和倾斜的RDD中独立出来的RDD(添加了随机数前缀的RDD)进行join操作，剩下的没有数据倾斜的RDD进行join操作，最后再使用union合并最终的结果

7、将正常的RDD进行扩容，添加随机数进行扩容之后进行统计

8、将以上几种方案进行组合使用



